{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_tryout_Ben.ipynb","provenance":[{"file_id":"1w2JgZX_8BaWCcpzjy40Iqt4XNuvw8L3M","timestamp":1611146580598}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nlfuS2-WAvds"},"source":["# Data Loader & text preprocessing\r\n"]},{"cell_type":"code","metadata":{"id":"xOBDv4mQ_zMS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611442682337,"user_tz":-60,"elapsed":233453,"user":{"displayName":"Bennie Veldhuijzen","photoUrl":"","userId":"13657742996875096655"}},"outputId":"063a8599-9db6-4aae-e9bf-dc39e42d8f6a"},"source":["#!unzip -qq crystal_ball_data.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["replace crystal_ball_data/test_violations/Article10/001-100551.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJpJK7-aMBtc","executionInfo":{"status":"ok","timestamp":1611442687182,"user_tz":-60,"elapsed":238278,"user":{"displayName":"Bennie Veldhuijzen","photoUrl":"","userId":"13657742996875096655"}},"outputId":"17829906-f92a-4b96-efce-0af9ec1dd169"},"source":["!pip install transformers\r\n","!pip install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNCCm6huJRZc","executionInfo":{"status":"ok","timestamp":1611442687183,"user_tz":-60,"elapsed":238270,"user":{"displayName":"Bennie Veldhuijzen","photoUrl":"","userId":"13657742996875096655"}},"outputId":"3dabd5e7-5dd2-4fdb-b3e0-38d1d5ace83c"},"source":["import torch\n","from __future__ import print_function\n","import re, glob, sys, time, os, random\n","from time import gmtime, strftime\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import LinearSVC\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","import pandas as pd\n","import warnings\n","from sklearn.model_selection import cross_val_predict, cross_val_score\n","from statistics import mean\n","from datetime import datetime\n","from time import time\n","import logging\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","import pprint\n","from random import shuffle\n","from typing import List\n","from typing import Tuple, List, Callable, Optional\n","from torch import FloatTensor, LongTensor\n","from torch import nn\n","from transformers import BertForSequenceClassification\n","from transformers import BertTokenizer\n","from typing import List\n","import random\n","import sklearn\n","from math import ceil\n","\n","pipeline = Pipeline([\n","\t('tfidf', TfidfVectorizer(analyzer='word')),\n","\t('clf', LinearSVC())\n","])\n","\n","\n","parameters = {\n","\t'tfidf__ngram_range': [(1,2),(1,1),(1,3),(1,4),(2,2),(2,3),(2,4),(3,3),(3,4),(4,4)],\n","\t#'tfidf__analyzer': ('word', 'char'),\n","\t'tfidf__lowercase': (True, False),\n","\t#'tfidf__max_df': (0.01, 1.0), # ignore words that occur as more than 1% of corpus\n","\t'tfidf__min_df': (1, 2, 3), # we need to see a word at least (once, twice, thrice)\n","\t'tfidf__use_idf': (False, True),\n","\t#'tfidf__sublinear_tf': (False, True),\n","\t'tfidf__binary': (False, True),\n","\t'tfidf__norm': (None, 'l1', 'l2'),\n","\t#'tfidf__max_features': (None, 2000, 5000),\n","\t'tfidf__stop_words': (None, 'english'),\n","\n","\t#'tfidfchar_ngram_range': ((1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(2,2),(2,3),(2,4),(2,5),(2,6),(3,3),(3,4),(3,5),(3,6),(4,4),(4,5),(4,6),(5,5),(5,6),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(7,7)),\n","\t\n","\t\n","\t'clf__C':(0.1, 1, 5)\n","}\n","\n","\n","def balance(Xtrain,Ytrain):\n","\t#v = Ytrain.count('violation')\n","   # nv = Ytrain.count('non-violation')\n","\t#print(v, nv)\n","\tv = [i for i,val in enumerate(Ytrain) if val=='violation']\n","\tnv = [i for i,val in enumerate(Ytrain) if val=='non-violation']\n","\tif len(nv) < len(v):\n","\t\tv = v[:len(nv)]\n","\t\tXtrain = [Xtrain[j] for j in v] + [Xtrain[i] for i in nv]\n","\t\tYtrain = [Ytrain[j] for j in v] + [Ytrain[i] for i in nv]\n","\tif len(nv) > len(v):\n","\t\tnv = nv[:len(v)]\n","\t\tXtrain = [Xtrain[j] for j in v] + [Xtrain[i] for i in nv]\n","\t\tYtrain = [Ytrain[j] for j in v] + [Ytrain[i] for i in nv]\n","\t\n","\t#print(Ytrain.count('violation'),Ytrain.count('non-violation'))\n","\t#print('LEN', len(Xtrain), len(Ytrain))\n","\treturn Xtrain, Ytrain\n","\t\n","\t\n","\n","def extract_text(starts, ends, cases, violation):\n","\tfacts = []\n","\tD = []\n","\tyears = []\n","\tfor case in cases:\n","\t\tcontline = ''\n","\t\tyear = 0\n","\t\twith open(case, 'r', encoding=\"mbcs\") as f:\n","\t\t\tfor line in f:\n","\t\t\t\t#print(line)\n","\t\t\t\tdat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n","\t\t\t\tif dat != None:\n","\t\t\t\t\tyear = int(dat.group(2))\n","\t\t\t\t\tbreak\n","\t\t\tif year>0:\n","\t\t\t\tyears.append(year)\n","\t\t\t\t#print(year)\n","\t\t\t\twr = 0\n","\t\t\t\tfor line in f:\n","\t\t\t\t\tif wr == 0:\n","\t\t\t\t\t\tif re.search(starts, line) != None:\n","\t\t\t\t\t\t\twr = 1\n","\t\t\t\t\tif wr == 1 and re.search(ends, line) == None:\n","\t\t\t\t\t\tcontline += line\n","\t\t\t\t\t\tcontline += '\\n'\n","\t\t\t\t\telif re.search(ends, line) != None:\n","\t\t\t\t\t\tbreak\n","\t\t\t\tfacts.append(contline)\n","\tfor i in range(len(facts)):\n","\t\tD.append((facts[i], violation, years[i])) \n","\treturn D\n","\n","def extract_parts(article, violation, part, path):\n","  # Path is the path to the folder that contains all the text files.\n","\tfrom os import listdir\n","\tfrom os.path import isfile, join\n","\tcases = [join(path, f) for f in listdir(path)]\n","\t# cases = glob.glob(path)\n","\t#print(cases)\n","\n","\t\t\n","\tfacts = []\n","\tD = []\n","\tyears = []\n","\t\n","\tif part == 'relevant_law':\n","\t\tfor case in cases:\n","\t\t\tyear = 0\n","\t\t\tcontline = ''\n","\t\t\twith open(case, 'r') as f:\n","\t\t\t\tfor line in f:\n","\t\t\t\t\tdat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n","\t\t\t\t\tif dat != None:\n","\t\t\t\t\t\t #date = dat.group(1)\n","\t\t\t\t\t\tyear = int(dat.group(2))\n","\t\t\t\t\t\tbreak\n","\t\t\t\tif year> 0:\n","\t\t\t\t\tyears.append(year)\n","\t\t\t\t\twr = 0\n","\t\t\t\t\tfor line in f:\n","\t\t\t\t\t\tif wr == 0:\n","\t\t\t\t\t\t\tif re.search('RELEVANT', line) != None:\n","\t\t\t\t\t\t\t\twr = 1\n","\t\t\t\t\t\tif wr == 1 and re.search('THE LAW', line) == None and re.search('PROCEEDINGS', line) == None:\n","\t\t\t\t\t\t\tcontline += line\n","\t\t\t\t\t\t\tcontline += '\\n'\n","\t\t\t\t\t\telif re.search('THE LAW', line) != None or re.search('PROCEEDINGS', line) != None:\n","\t\t\t\t\t\t\tbreak\n","\t\t\t\t\tfacts.append(contline)\n","\t\tfor i in range(len(facts)):\n","\t\t\tD.append((facts[i], violation, years[i]))\n","\t\t\n","\tif part == 'facts':\n","\t\tstarts = 'THE FACTS'\n","\t\tends ='THE LAW'\n","\t\tD = extract_text(starts, ends, cases, violation)\n","\tif part == 'circumstances':\n","\t\tstarts = 'CIRCUMSTANCES'\n","\t\tends ='RELEVANT'\n","\t\tD = extract_text(starts, ends, cases, violation)\n","\tif part == 'procedure':\n","\t\tstarts = 'PROCEDURE'\n","\t\tends ='THE FACTS'\n","\t\tD = extract_text(starts, ends, cases, violation)\n","\tif part == 'procedure+facts':\n","\t\tstarts = 'PROCEDURE'\n","\t\tends ='THE LAW'\n","\t\tD = extract_text(starts, ends, cases, violation)\n","\treturn D\n","\n","\n","def fetch(part, path, article):\n","  train_v = extract_parts(article, 'violation', part, path+'/train/'+article+'/violation/')\n","  train_nv = extract_parts(article, 'non-violation', part, path+'/train/'+article+'/non-violation/')\n","  test_v = extract_parts(article, 'violation', part, path+'/test20/'+article+'/violation/')\n","  test_nv = extract_parts(article, 'non-violation', part, path+'/test20/'+article+'/non-violation/')\n"," \n","  return train_v, train_nv, test_v, test_nv\n","\n","\n","def get_facts_dataset(articles: List[int], shuffle: bool = False):\n","  \"\"\"\n","  Returns a tuple of (training_data, training_labels, test_data, test labels)\n","  containing the data from the given articles. The 'data' fields are lists of strings\n","  that contain the FACTS part of the cases, while the 'labels' fields are also lists of\n","  strings containing either 'violation' or 'non-violation' for their respective data\n","  counterparts.\n","  :param articles: List of integers of article numbers.\n","  :param shuffle: Randomly shuffles the training set if True.\n","  \"\"\"\n","  path = \"crystal_ball_data\"\n","  traind = []\n","  trainl = []\n","  testd = []\n","  testl = []\n","  for i in articles:\n","    art = f\"Article{i}\"\n","    trv, trnv, tev, tenv = fetch(\"facts\", path, art)\n","    traind.extend([e[0] for e in trv] + [e[0] for e in trnv])\n","    trainl.extend([e[1] for e in trv] + [e[1] for e in trnv])\n","\n","    testd.extend([e[0] for e in tev] + [e[0] for e in tenv])\n","    testl.extend([e[1] for e in tev] + [e[1] for e in tenv])\n","  \n","  if shuffle:\n","    c = list(zip(traind, trainl))\n","    random.shuffle(c)\n","\n","    traind, trainl = zip(*c)\n","  \n","  return traind, trainl, testd, testl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CfrvuKcQMOjz"},"source":["pre_trained_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased')\r\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Onp-fReFbGTU"},"source":["# Train + Test functions"]},{"cell_type":"code","metadata":{"id":"M687ZetqbKGf"},"source":["def train(model: torch.nn.Module, data: torch.Tensor, labels: torch.Tensor, batch_size: int, optimizer, verbose=False):\r\n","  \"\"\"\r\n","  Trains the network for one epoch.\r\n","  \"\"\"\r\n","  # Set to training mode.\r\n","  model.train(True)\r\n","\r\n","  for start_i in range(0, len(data), batch_size):\r\n","    x = data[start_i:start_i+batch_size]\r\n","    y = labels[start_i:start_i+batch_size]\r\n","\r\n","    y = y.to(torch.float32)\r\n","    out = model(x)\r\n","    out = out.squeeze(-1)\r\n","    #h_n.detach_()\r\n","    #h_c.detach_()\r\n","    #print(out.shape)\r\n","    #print(y.shape)\r\n","\r\n","    #print(y[1].item())\r\n","    #print(out[1].item())\r\n","\r\n","    loss = torch.nn.functional.binary_cross_entropy(input=out, target=y)\r\n","    #lossfunc = torch.nn.BCELoss()\r\n","    #loss = lossfunc(out, y)\r\n","    if verbose:\r\n","      print(f\"loss: {loss.item()}\")\r\n","    model.zero_grad()\r\n","    loss.backward(retain_graph = True)\r\n","    optimizer.step()\r\n","\r\n","\r\n","def test(model: torch.nn.Module, data: torch.Tensor, labels: torch.Tensor, batch_size: int = 32) -> float:\r\n","  \"\"\"\r\n","  Calculates classification accuracy on the dataset and returns the result.\r\n","  \"\"\"\r\n","  model.train(False)\r\n","  accuracy = 0\r\n","  weight = 0\r\n","  for start_i in range(0, len(data), batch_size):\r\n","    x = data[start_i:start_i+batch_size]\r\n","    y = labels[start_i:start_i+batch_size]\r\n","    weight += len(x)\r\n","    with torch.no_grad():\r\n","      y_hat = torch.round(model(x)).cpu()\r\n","    #print(y_hat)\r\n","    accuracy += len(x) * sklearn.metrics.accuracy_score(y, y_hat)\r\n","  return accuracy / weight\r\n","\r\n","def save(model, optimizer, epoch):\r\n","  torch.save({\r\n","          'model_state_dict': model.state_dict(),\r\n","          'optimizer_state_dict': optimizer.state_dict(),\r\n","          'epoch': epoch\r\n","      }, f\"/content/drive/My Drive/PR checkpoints/checkpoint_e{epoch}.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIYotNcJDCL2"},"source":["# Tokenize and vectorize"]},{"cell_type":"code","metadata":{"id":"muHcJJUuDGRE"},"source":["# We can use all these article numbers: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18] \r\n","# For training\r\n","article_numbers = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18]\r\n","training_data, training_labels, test_data, test_labels = get_facts_dataset(article_numbers, shuffle=True)\r\n","\r\n","# Convert it to token IDs.\r\n","training_data_token = tokenizer(training_data, return_tensors='pt', padding=True, truncation=True)\r\n","# RNN we only need the IDs.\r\n","training_data_token[\"input_ids\"]\r\n","# Now convert the tokens into vectors.\r\n","training_vectors = pre_trained_bert.bert.embeddings(training_data_token[\"input_ids\"])\r\n","    \r\n","import torch\r\n","# Convert labels to 0 and 1\r\n","training_labels_binary = torch.tensor([0 if l == \"violation\" else 1 for l in training_labels], dtype=torch.long)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qn827G3JIo05"},"source":["#(everytime when you swap )\r\n","###############################################################################################\r\n","import torch\r\n","\r\n","# We can use all these article numbers: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18] \r\n","# For training\r\n","article_numbers = [18]\r\n","void1, void2, test_data, test_labels = get_facts_dataset(article_numbers, shuffle=True)\r\n","# Convert it to token IDs. (everytime when you swap )\r\n","test_data_token = tokenizer(test_data, return_tensors='pt', padding=True, truncation=True)\r\n","# RNN we only need the IDs.\r\n","test_data_token[\"input_ids\"]\r\n","# Now convert the tokens into vectors.\r\n","test_vectors = pre_trained_bert.bert.embeddings(test_data_token[\"input_ids\"])\r\n","# Convert labels to 0 and 1\r\n","test_labels_binary = torch.tensor([0 if l == \"violation\" else 1 for l in test_labels], dtype=torch.long)\r\n","\r\n","###############################################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqT5BMb2PZ1x"},"source":["# RNN model"]},{"cell_type":"code","metadata":{"id":"-_XU6yGjPdMj"},"source":["#change hidden layer size for higher accuracy\r\n","hidden_layer_size = 256\r\n","\r\n","class RNN(nn.Module):\r\n","    def __init__(self):\r\n","        super(RNN, self).__init__()\r\n","\r\n","        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\r\n","            input_size= 768,\r\n","            hidden_size=hidden_layer_size,         # rnn hidden unit\r\n","            num_layers=1,           # number of rnn layer\r\n","            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\r\n","        )\r\n","\r\n","        self.out = nn.Linear(hidden_layer_size, 1)\r\n","\r\n","    def forward(self, x):\r\n","        # x shape (batch, time_step, input_size)\r\n","        # r_out shape (batch, time_step, output_size)\r\n","        # h_n shape (n_layers, batch, hidden_size)\r\n","        # h_c shape (n_layers, batch, hidden_size)\r\n","        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\r\n","\r\n","        # choose r_out at the last time step\r\n","        out = self.out(r_out[:, -1, :])\r\n","        return torch.nn.functional.sigmoid(out)\r\n","\r\n","rnn = RNN()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7DKuL7FOQQev"},"source":["# Train and test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytsJobyUQc3X","executionInfo":{"status":"ok","timestamp":1611450565502,"user_tz":-60,"elapsed":2952489,"user":{"displayName":"Bennie Veldhuijzen","photoUrl":"","userId":"13657742996875096655"}},"outputId":"83f5f69f-6c90-4984-a294-4b044a020d8f"},"source":["#change batch size and learning rate to get high accuracy\r\n","batch_size = 32\r\n","learning_rate = 0.0001\r\n","optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate) \r\n","\r\n","\r\n","print(f\"Size of the training data is: {len(training_vectors)}. Completing an epoch with batch size of {batch_size} will take {ceil(len(training_vectors) / batch_size)} iterations.\")\r\n","acc = test(rnn, test_vectors, test_labels_binary, batch_size=batch_size)\r\n","print(f\"| Test accuracy: {acc*100:.3f}%\")\r\n","for epoch in range(8):\r\n","    train(rnn, training_vectors, training_labels_binary, batch_size=batch_size, optimizer=optimizer, verbose=False)\r\n","    acc = test(rnn, test_vectors, test_labels_binary, batch_size=batch_size)\r\n","    #save(model=rnn, optimizer=optimizer, epoch=epoch+1)\r\n","    print(f\"=== Epoch {epoch+1} completed. ===\")\r\n","    print(f\"| Test accuracy: {acc*100:.3f}%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of the training data is: 800. Completing an epoch with batch size of 32 will take 25 iterations.\n","| Test accuracy: 46.296%\n","=== Epoch 1 completed. ===\n","| Test accuracy: 46.296%\n","=== Epoch 2 completed. ===\n","| Test accuracy: 51.852%\n","=== Epoch 3 completed. ===\n","| Test accuracy: 53.704%\n","=== Epoch 4 completed. ===\n","| Test accuracy: 51.852%\n","=== Epoch 5 completed. ===\n","| Test accuracy: 50.000%\n","=== Epoch 6 completed. ===\n","| Test accuracy: 46.296%\n","=== Epoch 7 completed. ===\n","| Test accuracy: 48.148%\n","=== Epoch 8 completed. ===\n","| Test accuracy: 55.556%\n","=== Epoch 9 completed. ===\n","| Test accuracy: 51.852%\n","=== Epoch 10 completed. ===\n","| Test accuracy: 48.148%\n"],"name":"stdout"}]}]}