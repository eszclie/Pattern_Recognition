{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -qq crystal_ball_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\benni\\anaconda3\\lib\\site-packages (4.2.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\benni\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\benni\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\benni\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: requests in c:\\users\\benni\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\benni\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\benni\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\benni\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in c:\\users\\benni\\anaconda3\\lib\\site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\benni\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\benni\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\benni\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\benni\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benni\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\benni\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\benni\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\benni\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: torch in c:\\users\\benni\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\benni\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\benni\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from __future__ import print_function\n",
    "import re, glob, sys, time, os, random\n",
    "from time import gmtime, strftime\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from statistics import mean\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import pprint\n",
    "from random import shuffle\n",
    "from typing import List\n",
    "from typing import Tuple, List, Callable, Optional\n",
    "from torch import FloatTensor, LongTensor\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from typing import List\n",
    "import random\n",
    "import sklearn\n",
    "from math import ceil\n",
    "\n",
    "pipeline = Pipeline([\n",
    "\t('tfidf', TfidfVectorizer(analyzer='word')),\n",
    "\t('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "\t'tfidf__ngram_range': [(1,2),(1,1),(1,3),(1,4),(2,2),(2,3),(2,4),(3,3),(3,4),(4,4)],\n",
    "\t#'tfidf__analyzer': ('word', 'char'),\n",
    "\t'tfidf__lowercase': (True, False),\n",
    "\t#'tfidf__max_df': (0.01, 1.0), # ignore words that occur as more than 1% of corpus\n",
    "\t'tfidf__min_df': (1, 2, 3), # we need to see a word at least (once, twice, thrice)\n",
    "\t'tfidf__use_idf': (False, True),\n",
    "\t#'tfidf__sublinear_tf': (False, True),\n",
    "\t'tfidf__binary': (False, True),\n",
    "\t'tfidf__norm': (None, 'l1', 'l2'),\n",
    "\t#'tfidf__max_features': (None, 2000, 5000),\n",
    "\t'tfidf__stop_words': (None, 'english'),\n",
    "\n",
    "\t#'tfidfchar_ngram_range': ((1,1),(1,2),(1,3),(1,4),(1,5),(1,6),(2,2),(2,3),(2,4),(2,5),(2,6),(3,3),(3,4),(3,5),(3,6),(4,4),(4,5),(4,6),(5,5),(5,6),(1,7),(2,7),(3,7),(4,7),(5,7),(6,7),(7,7)),\n",
    "\t\n",
    "\t\n",
    "\t'clf__C':(0.1, 1, 5)\n",
    "}\n",
    "\n",
    "\n",
    "def balance(Xtrain,Ytrain):\n",
    "\t#v = Ytrain.count('violation')\n",
    "   # nv = Ytrain.count('non-violation')\n",
    "\t#print(v, nv)\n",
    "\tv = [i for i,val in enumerate(Ytrain) if val=='violation']\n",
    "\tnv = [i for i,val in enumerate(Ytrain) if val=='non-violation']\n",
    "\tif len(nv) < len(v):\n",
    "\t\tv = v[:len(nv)]\n",
    "\t\tXtrain = [Xtrain[j] for j in v] + [Xtrain[i] for i in nv]\n",
    "\t\tYtrain = [Ytrain[j] for j in v] + [Ytrain[i] for i in nv]\n",
    "\tif len(nv) > len(v):\n",
    "\t\tnv = nv[:len(v)]\n",
    "\t\tXtrain = [Xtrain[j] for j in v] + [Xtrain[i] for i in nv]\n",
    "\t\tYtrain = [Ytrain[j] for j in v] + [Ytrain[i] for i in nv]\n",
    "\t\n",
    "\t#print(Ytrain.count('violation'),Ytrain.count('non-violation'))\n",
    "\t#print('LEN', len(Xtrain), len(Ytrain))\n",
    "\treturn Xtrain, Ytrain\n",
    "\t\n",
    "\t\n",
    "\n",
    "def extract_text(starts, ends, cases, violation):\n",
    "\tfacts = []\n",
    "\tD = []\n",
    "\tyears = []\n",
    "\tfor case in cases:\n",
    "\t\tcontline = ''\n",
    "\t\tyear = 0\n",
    "\t\twith open(case, 'r', encoding=\"mbcs\") as f:\n",
    "\t\t\tfor line in f:\n",
    "\t\t\t\t#print(line)\n",
    "\t\t\t\tdat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "\t\t\t\tif dat != None:\n",
    "\t\t\t\t\tyear = int(dat.group(2))\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tif year>0:\n",
    "\t\t\t\tyears.append(year)\n",
    "\t\t\t\t#print(year)\n",
    "\t\t\t\twr = 0\n",
    "\t\t\t\tfor line in f:\n",
    "\t\t\t\t\tif wr == 0:\n",
    "\t\t\t\t\t\tif re.search(starts, line) != None:\n",
    "\t\t\t\t\t\t\twr = 1\n",
    "\t\t\t\t\tif wr == 1 and re.search(ends, line) == None:\n",
    "\t\t\t\t\t\tcontline += line\n",
    "\t\t\t\t\t\tcontline += '\\n'\n",
    "\t\t\t\t\telif re.search(ends, line) != None:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tfacts.append(contline)\n",
    "\tfor i in range(len(facts)):\n",
    "\t\tD.append((facts[i], violation, years[i])) \n",
    "\treturn D\n",
    "\n",
    "def extract_parts(article, violation, part, path):\n",
    "  # Path is the path to the folder that contains all the text files.\n",
    "\tfrom os import listdir\n",
    "\tfrom os.path import isfile, join\n",
    "\tcases = [join(path, f) for f in listdir(path)]\n",
    "\t# cases = glob.glob(path)\n",
    "\t#print(cases)\n",
    "\n",
    "\t\t\n",
    "\tfacts = []\n",
    "\tD = []\n",
    "\tyears = []\n",
    "\t\n",
    "\tif part == 'relevant_law':\n",
    "\t\tfor case in cases:\n",
    "\t\t\tyear = 0\n",
    "\t\t\tcontline = ''\n",
    "\t\t\twith open(case, 'r') as f:\n",
    "\t\t\t\tfor line in f:\n",
    "\t\t\t\t\tdat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "\t\t\t\t\tif dat != None:\n",
    "\t\t\t\t\t\t #date = dat.group(1)\n",
    "\t\t\t\t\t\tyear = int(dat.group(2))\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif year> 0:\n",
    "\t\t\t\t\tyears.append(year)\n",
    "\t\t\t\t\twr = 0\n",
    "\t\t\t\t\tfor line in f:\n",
    "\t\t\t\t\t\tif wr == 0:\n",
    "\t\t\t\t\t\t\tif re.search('RELEVANT', line) != None:\n",
    "\t\t\t\t\t\t\t\twr = 1\n",
    "\t\t\t\t\t\tif wr == 1 and re.search('THE LAW', line) == None and re.search('PROCEEDINGS', line) == None:\n",
    "\t\t\t\t\t\t\tcontline += line\n",
    "\t\t\t\t\t\t\tcontline += '\\n'\n",
    "\t\t\t\t\t\telif re.search('THE LAW', line) != None or re.search('PROCEEDINGS', line) != None:\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tfacts.append(contline)\n",
    "\t\tfor i in range(len(facts)):\n",
    "\t\t\tD.append((facts[i], violation, years[i]))\n",
    "\t\t\n",
    "\tif part == 'facts':\n",
    "\t\tstarts = 'THE FACTS'\n",
    "\t\tends ='THE LAW'\n",
    "\t\tD = extract_text(starts, ends, cases, violation)\n",
    "\tif part == 'circumstances':\n",
    "\t\tstarts = 'CIRCUMSTANCES'\n",
    "\t\tends ='RELEVANT'\n",
    "\t\tD = extract_text(starts, ends, cases, violation)\n",
    "\tif part == 'procedure':\n",
    "\t\tstarts = 'PROCEDURE'\n",
    "\t\tends ='THE FACTS'\n",
    "\t\tD = extract_text(starts, ends, cases, violation)\n",
    "\tif part == 'procedure+facts':\n",
    "\t\tstarts = 'PROCEDURE'\n",
    "\t\tends ='THE LAW'\n",
    "\t\tD = extract_text(starts, ends, cases, violation)\n",
    "\treturn D\n",
    "\n",
    "\n",
    "def fetch(part, path, article):\n",
    "  train_v = extract_parts(article, 'violation', part, path+'/train/'+article+'/violation/')\n",
    "  train_nv = extract_parts(article, 'non-violation', part, path+'/train/'+article+'/non-violation/')\n",
    "  test_v = extract_parts(article, 'violation', part, path+'/test20/'+article+'/violation/')\n",
    "  test_nv = extract_parts(article, 'non-violation', part, path+'/test20/'+article+'/non-violation/')\n",
    " \n",
    "  return train_v, train_nv, test_v, test_nv\n",
    "\n",
    "\n",
    "def get_facts_dataset(articles: List[int], shuffle: bool = False):\n",
    "  \"\"\"\n",
    "  Returns a tuple of (training_data, training_labels, test_data, test labels)\n",
    "  containing the data from the given articles. The 'data' fields are lists of strings\n",
    "  that contain the FACTS part of the cases, while the 'labels' fields are also lists of\n",
    "  strings containing either 'violation' or 'non-violation' for their respective data\n",
    "  counterparts.\n",
    "  :param articles: List of integers of article numbers.\n",
    "  :param shuffle: Randomly shuffles the training set if True.\n",
    "  \"\"\"\n",
    "  path = \"crystal_ball_data\"\n",
    "  traind = []\n",
    "  trainl = []\n",
    "  testd = []\n",
    "  testl = []\n",
    "  for i in articles:\n",
    "    art = f\"Article{i}\"\n",
    "    trv, trnv, tev, tenv = fetch(\"facts\", path, art)\n",
    "    traind.extend([e[0] for e in trv] + [e[0] for e in trnv])\n",
    "    trainl.extend([e[1] for e in trv] + [e[1] for e in trnv])\n",
    "\n",
    "    testd.extend([e[0] for e in tev] + [e[0] for e in tenv])\n",
    "    testl.extend([e[1] for e in tev] + [e[1] for e in tenv])\n",
    "  \n",
    "  if shuffle:\n",
    "    c = list(zip(traind, trainl))\n",
    "    random.shuffle(c)\n",
    "\n",
    "    traind, trainl = zip(*c)\n",
    "  \n",
    "  return traind, trainl, testd, testl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pre_trained_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, data: torch.Tensor, labels: torch.Tensor, batch_size: int, optimizer, verbose=False):\n",
    "  \"\"\"\n",
    "  Trains the network for one epoch.\n",
    "  \"\"\"\n",
    "  # Set to training mode.\n",
    "  model.train(True)\n",
    "\n",
    "  for start_i in range(0, len(data), batch_size):\n",
    "    x = data[start_i:start_i+batch_size]\n",
    "    y = labels[start_i:start_i+batch_size]\n",
    "\n",
    "    y = y.to(torch.float32)\n",
    "    out = model(x)\n",
    "    out = out.squeeze(-1)\n",
    "    #h_n.detach_()\n",
    "    #h_c.detach_()\n",
    "    #print(out.shape)\n",
    "    #print(y.shape)\n",
    "\n",
    "    #print(y[1].item())\n",
    "    #print(out[1].item())\n",
    "\n",
    "    loss = torch.nn.functional.binary_cross_entropy(input=out, target=y)\n",
    "    #lossfunc = torch.nn.BCELoss()\n",
    "    #loss = lossfunc(out, y)\n",
    "    if verbose:\n",
    "      print(f\"loss: {loss.item()}\")\n",
    "    model.zero_grad()\n",
    "    loss.backward(retain_graph = True)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test(model: torch.nn.Module, data: torch.Tensor, labels: torch.Tensor, batch_size: int = 32) -> float:\n",
    "  \"\"\"\n",
    "  Calculates classification accuracy on the dataset and returns the result.\n",
    "  \"\"\"\n",
    "  model.train(False)\n",
    "  accuracy = 0\n",
    "  weight = 0\n",
    "  for start_i in range(0, len(data), batch_size):\n",
    "    x = data[start_i:start_i+batch_size]\n",
    "    y = labels[start_i:start_i+batch_size]\n",
    "    weight += len(x)\n",
    "    with torch.no_grad():\n",
    "      y_hat = torch.round(model(x)).cpu()\n",
    "    #print(y_hat)\n",
    "    accuracy += len(x) * sklearn.metrics.accuracy_score(y, y_hat)\n",
    "  return accuracy / weight\n",
    "\n",
    "def save(model, optimizer, epoch):\n",
    "  torch.save({\n",
    "          'model_state_dict': model.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict(),\n",
    "          'epoch': epoch\n",
    "      }, f\"/content/drive/My Drive/PR checkpoints/checkpoint_e{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use all these article numbers: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18] \n",
    "# For training\n",
    "article_numbers = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18]\n",
    "training_data, training_labels, test_data, test_labels = get_facts_dataset(article_numbers, shuffle=True)\n",
    "\n",
    "# Convert it to token IDs.\n",
    "training_data_token = tokenizer(training_data, return_tensors='pt', padding=True, truncation=True)\n",
    "# RNN we only need the IDs.\n",
    "training_data_token[\"input_ids\"]\n",
    "# Now convert the tokens into vectors.\n",
    "training_vectors = pre_trained_bert.bert.embeddings(training_data_token[\"input_ids\"])\n",
    "    \n",
    "import torch\n",
    "# Convert labels to 0 and 1\n",
    "training_labels_binary = torch.tensor([0 if l == \"violation\" else 1 for l in training_labels], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(everytime when you swap )\n",
    "###############################################################################################\n",
    "import torch\n",
    "\n",
    "# We can use all these article numbers: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 18] \n",
    "# For training\n",
    "article_numbers = [10]\n",
    "void1, void2, test_data, test_labels = get_facts_dataset(article_numbers, shuffle=True)\n",
    "# Convert it to token IDs. (everytime when you swap )\n",
    "test_data_token = tokenizer(test_data, return_tensors='pt', padding=True, truncation=True)\n",
    "# RNN we only need the IDs.\n",
    "test_data_token[\"input_ids\"]\n",
    "# Now convert the tokens into vectors.\n",
    "test_vectors = pre_trained_bert.bert.embeddings(test_data_token[\"input_ids\"])\n",
    "# Convert labels to 0 and 1\n",
    "test_labels_binary = torch.tensor([0 if l == \"violation\" else 1 for l in test_labels], dtype=torch.long)\n",
    "\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change hidden layer size for higher accuracy\n",
    "hidden_layer_size = 256\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size= 768,\n",
    "            hidden_size=hidden_layer_size,         # rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(hidden_layer_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return torch.nn.functional.sigmoid(out)\n",
    "\n",
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training data is: 3166. Completing an epoch with batch size of 32 will take 99 iterations.\n",
      "| Test accuracy: 52.830%\n",
      "=== Epoch 1 completed. ===\n",
      "| Test accuracy: 43.396%\n",
      "=== Epoch 2 completed. ===\n",
      "| Test accuracy: 43.396%\n",
      "=== Epoch 3 completed. ===\n",
      "| Test accuracy: 41.509%\n",
      "=== Epoch 4 completed. ===\n",
      "| Test accuracy: 52.830%\n",
      "=== Epoch 5 completed. ===\n",
      "| Test accuracy: 52.830%\n",
      "=== Epoch 6 completed. ===\n",
      "| Test accuracy: 50.943%\n",
      "=== Epoch 7 completed. ===\n",
      "| Test accuracy: 54.717%\n",
      "=== Epoch 8 completed. ===\n",
      "| Test accuracy: 54.717%\n"
     ]
    }
   ],
   "source": [
    "#change batch size and learning rate to get high accuracy\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate) \n",
    "\n",
    "\n",
    "print(f\"Size of the training data is: {len(training_vectors)}. Completing an epoch with batch size of {batch_size} will take {ceil(len(training_vectors) / batch_size)} iterations.\")\n",
    "acc = test(rnn, test_vectors, test_labels_binary, batch_size=batch_size)\n",
    "print(f\"| Test accuracy: {acc*100:.3f}%\")\n",
    "for epoch in range(8):\n",
    "    train(rnn, training_vectors, training_labels_binary, batch_size=batch_size, optimizer=optimizer, verbose=False)\n",
    "    acc = test(rnn, test_vectors, test_labels_binary, batch_size=batch_size)\n",
    "    #save(model=rnn, optimizer=optimizer, epoch=epoch+1)\n",
    "    print(f\"=== Epoch {epoch+1} completed. ===\")\n",
    "    print(f\"| Test accuracy: {acc*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
